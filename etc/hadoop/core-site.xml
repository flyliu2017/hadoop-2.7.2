<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://singlenode.ustc.edu:9000</value>
    </property>

    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/hadoop-2.7.2/tmp</value>
    </property>

    <property>
        <name>hadoop.http.filter.initializers</name>
        <value>org.apache.hadoop.security.AuthenticationFilterInitializer</value>
        <description>A comma separated list of class names. Each class in the list
            must extend org.apache.hadoop.http.FilterInitializer. The corresponding
            Filter will be initialized. Then, the Filter will be applied to all user
            facing jsp and servlet web pages. The ordering of the list defines the
            ordering of the filters.
        </description>
    </property>

    <property>
        <name>hadoop.http.authentication.type</name>
        <value>simple</value>
        <description>Possible values are simple (no authentication), and kerberos
        </description>
    </property>

    <property>
        <name>hadoop.http.authentication.token.validity</name>
        <value>36000</value>
        <description>
            Indicates how long (in seconds) an authentication token is valid before it has
            to be renewed.
        </description>
    </property>

    <property>
        <name>hadoop.http.authentication.cookie.domain</name>
        <value>ustc.edu</value>
        <description>
            The domain to use for the HTTP cookie that stores the authentication token.
            In order to authentiation to work correctly across all Hadoop nodes web-consoles
            the domain must be correctly set.
            IMPORTANT: when using IP addresses, browsers ignore cookies with domain settings.
            For this setting to work properly all nodes in the cluster must be configured
            to generate URLs with hostname.domain names on it.
        </description>
    </property>

    <property>
        <name>hadoop.http.authentication.signature.secret.file</name>
        <value>/etc/security/http_secret</value>
        <description>
            The signature secret for signing the authentication tokens.
            The same secret should be used for JT/NN/DN/TT configurations.
        </description>
    </property>

    <property>
        <name>hadoop.http.authentication.simple.anonymous.allowed</name>
        <value>false</value>
        <description>
            Indicates if anonymous requests are allowed when using 'simple' authentication.
        </description>
    </property>

    <property>
        <name>hadoop.http.authentication.kerberos.principal</name>
        <value>HTTP/_HOST@SINGLENODE.COM</value>
        <description>
            Indicates the Kerberos principal to be used for HTTP endpoint.
            The principal MUST start with 'HTTP/' as per Kerberos HTTP SPNEGO specification.
        </description>
    </property>

    <property>
        <name>hadoop.http.authentication.kerberos.keytab</name>
        <value>/etc/security/keytab/spnego.service.keytab</value>
        <description>
            Location of the keytab file with the credentials for the principal.
            Referring to the same keytab file Oozie uses for its Kerberos credentials for Hadoop.
        </description>
    </property>

    <property>
        <name>hadoop.security.auth_to_local</name>
        <value>
            RULE:[2:$1@$0]([snjd]n@SINGLENODE.COM)s/.*/hdfs/
            RULE:[2:$1@$0](hdfs@SINGLENODE.COM)s/.*/hdfs/
            RULE:[2:$1@$0](HTTP@SINGLENODE.COM)s/.*/http/
            RULE:[2:$1@$0]([rn]m@SINGLENODE.COM)s/.*/yarn/
            RULE:[2:$1@$0](jhs@SINGLENODE.COM)s/.*/mapred/
            RULE:[1:$1@$0](^hue@.*$)s/^.*$/nobody/
            RULE:[1:$1@$0](^sentry@.*$)s/^.*$/nobody/
            RULE:[1:$1@$0](^hive@.*$)s/^.*$/nobody/
            RULE:[1:$1@$0](^oozie@.*$)s/^.*$/nobody/
            RULE:[1:$1@$0](^yarn@.*$)s/^.*$/nobody/
            RULE:[1:$1@$0](^mapred@.*$)s/^.*$/nobody/
            RULE:[1:$1@$0](^hdfs@.*$)s/^.*$/nobody/
            RULE:[1:$1@$0](^zookeeper@.*)s/^.*$/nobody/
            RULE:[1:$1@$0](^httpfs@.*$)s/^.*$/nobody/
            RULE:[1:$1@$0](^HTTP@.*$)s/^.*$/nobody/
            RULE:[2:$1/$2@$0](^.*$)s/^.*$/nobody/
            DEFAULT
        </value>
        <description>Maps kerberos principals to local user names</description>
    </property>

    <property>
        <name>hadoop.security.authorization</name>
        <value>true</value>
        <description>Is service-level authorization enabled?</description>
    </property>

    <property>
        <name>hadoop.security.authentication</name>
        <value>simple</value>
        <description>Possible values are simple (no authentication), and kerberos
        </description>
    </property>

    <!--<property>-->
    <!--<name>hadoop.proxyuser.knox.groups</name>-->
    <!--<value>users</value>-->
    <!--</property>-->

    <!--<property>-->
    <!--<name>hadoop.proxyuser.knox.hosts</name>-->
    <!--<value>Knox.EXAMPLE.COM</value>-->
    <!--</property>-->

    <!--<property>-->
    <!--<name>hadoop.proxyuser.hdfs.groups</name>-->
    <!--<value>root,hadoop,user</value>-->
    <!--<description>-->
    <!--The 'nfsserver' user is allowed to proxy all members of the 'users-group1' and-->
    <!--'users-group2' groups. Note that in most cases you will need to include the-->
    <!--group "root" because the user "root" (which usually belonges to "root" group) will-->
    <!--generally be the user that initially executes the mount on the NFS client system.-->
    <!--Set this to '*' to allow nfsserver user to proxy any group.-->
    <!--</description>-->
    <!--</property>-->

    <!--<property>-->
    <!--<name>hadoop.proxyuser.hdfs.hosts</name>-->
    <!--<value>singlenode.ustc.edu</value>-->
    <!--<description>-->
    <!--This is the host where the nfs gateway is running. Set this to '*' to allow-->
    <!--requests from any hosts to be proxied.-->
    <!--</description>-->
    <!--</property>-->
    <!--<property>-->
    <!--<name>hadoop.proxyuser.hdfs/singlenode.ustc.edu@SINGLENODE.COM.groups</name>-->
    <!--<value>*</value>-->
    <!--<description>-->
    <!--The 'nfsserver' user is allowed to proxy all members of the 'users-group1' and-->
    <!--'users-group2' groups. Note that in most cases you will need to include the-->
    <!--group "root" because the user "root" (which usually belonges to "root" group) will-->
    <!--generally be the user that initially executes the mount on the NFS client system.-->
    <!--Set this to '*' to allow nfsserver user to proxy any group.-->
    <!--</description>-->
    <!--</property>-->

    <!--<property>-->
    <!--<name>hadoop.proxyuser.hdfs/singlenode.ustc.edu@SINGLENODE.COM.hosts</name>-->
    <!--<value>*</value>-->
    <!--<description>-->
    <!--This is the host where the nfs gateway is running. Set this to '*' to allow-->
    <!--requests from any hosts to be proxied.-->
    <!--</description>-->
    <!--</property>-->

    <property>
        <name>hadoop.proxyuser.hdfs.groups</name>
        <value>*</value>
        <description>
            The 'nfsserver' user is allowed to proxy all members of the 'users-group1' and
            'users-group2' groups. Note that in most cases you will need to include the
            group "root" because the user "root" (which usually belonges to "root" group) will
            generally be the user that initially executes the mount on the NFS client system.
            Set this to '*' to allow nfsserver user to proxy any group.
        </description>
    </property>

    <property>
        <name>hadoop.proxyuser.hdfs.hosts</name>
        <value>*</value>
        <description>
            This is the host where the nfs gateway is running. Set this to '*' to allow
            requests from any hosts to be proxied.
        </description>
    </property>
</configuration>
